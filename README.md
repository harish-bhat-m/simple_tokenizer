# Simple Tokenizer
# Simple Tokenizer
This is a simple tokenizer designed in Python Programming Language. Tokenizer
plays a crucial role in functioning the large language model. The main aim of
tokenizer is to convert the text into format that models can understand. 

In a nutshell tokenizer is software that breaks down the text into smaller text 
called tokens. The tokens can be a word or sub word or even can be characters.
The process of tokenization transforms the text into a structured format that LLM 
can process.

In this example of tokenization, we will process the large text breaks them into
word, punctuations etc. Then we will encode the word using simple encoding process
(each word will be numbered). This is nothing, but we are feeding the system with the 
training data. Once we feed the training data into the tokenizer, out tokenizer is ready
to encode the given text in defined encoding and decode the text into human-readable form.

